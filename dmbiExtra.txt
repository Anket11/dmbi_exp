import pandas as pd
import numpy as np

# Calculate the entropy of a set of labels
def entropy(labels):
    n = len(labels)
    _, counts = np.unique(labels, return_counts=True)
    probabilities = counts / n
    entropy = -np.sum(probabilities * np.log2(probabilities))
    return entropy

# Calculate the information gain of a feature
def information_gain(X, y, feature, threshold):
    left_labels = y[X[:, feature] <= threshold]
    right_labels = y[X[:, feature] > threshold]
    left_entropy = entropy(left_labels)
    right_entropy = entropy(right_labels)
    n = len(y)
    left_weight = len(left_labels) / n
    right_weight = len(right_labels) / n
    information_gain = entropy(y) - left_weight * left_entropy - right_weight * right_entropy
    return information_gain

# Find the best feature and threshold to split the data
def find_best_split(X, y):
    best_feature = None
    best_threshold = None
    best_gain = 0
    for feature in range(X.shape[1]):
        thresholds = np.unique(X[:, feature])
        for threshold in thresholds:
            gain = information_gain(X, y, feature, threshold)
            if gain > best_gain:
                best_feature = feature
                best_threshold = threshold
                best_gain = gain
    return best_feature, best_threshold, best_gain

# Build the decision tree recursively
def build_tree(X, y, max_depth=10, min_samples_split=2, gain_threshold=0, depth=0):
    # Stop splitting when the maximum depth is reached or the node contains too few samples
    if depth >= max_depth or len(y) < min_samples_split:
        prediction = np.bincount(y).argmax()
        return prediction

    # Stop splitting when all labels are the same
    if len(np.unique(y)) == 1:
        prediction = y[0]
        return prediction

    # Find the best feature and threshold to split the data
    best_feature, best_threshold, best_gain = find_best_split(X, y)

    # Stop splitting when the information gain is too low
    if best_gain < gain_threshold:
        prediction = np.bincount(y).argmax()
        return prediction

    # Split the data and build the left and right subtrees recursively
    left_indices = X[:, best_feature] <= best_threshold
    right_indices = X[:, best_feature] > best_threshold
    left_tree = build_tree(X[left_indices], y[left_indices], max_depth, min_samples_split, gain_threshold, depth + 1)
    right_tree = build_tree(X[right_indices], y[right_indices], max_depth, min_samples_split, gain_threshold, depth + 1)

    # Return the decision tree node
    decision_node = {'feature': best_feature, 'threshold': best_threshold, 'left': left_tree, 'right': right_tree}
    return decision_node

# Predict the label of a sample using the decision tree
def predict_sample(sample, tree):
    if isinstance(tree, dict):
        feature = tree['feature']
        threshold = tree['threshold']
        if sample[feature] <= threshold:
            return predict_sample(sample, tree['left'])
        else:
            return predict_sample(sample, tree['right'])
    else:
        return tree

# Predict the labels of a set of samples using the decision tree
def predict(X, tree):
    predictions = []
    for sample in X:
        prediction = predict_sample(sample, tree)
        predictions.append(prediction)
   return np.array(predictions)





Hello eh eh

def apriori(transactions, min_support=0.5):
    """
    Computes frequent itemsets using the Apriori algorithm.
    
    Args:
    - transactions: A list of transactions, where each transaction is a list of items.
    - min_support: The minimum support threshold.
    
    Returns:
    - A dictionary of frequent itemsets, where each key is an itemset and each value is the support.
    """
    
    # Count the frequency of each item in the dataset
    item_counts = {}
    for transaction in transactions:
        for item in transaction:
            if item not in item_counts:
                item_counts[item] = 0
            item_counts[item] += 1
    
    # Filter out infrequent items
    frequent_items = {item: count for item, count in item_counts.items() if count / len(transactions) >= min_support}
    
    # Generate candidate itemsets of length 2 or more
    candidate_itemsets = set(frozenset([item]) for item in frequent_items)
    k = 2
    while True:
        candidate_itemsets = set(frozenset(itemset1.union(itemset2)) for itemset1 in candidate_itemsets for itemset2 in candidate_itemsets if len(itemset1.union(itemset2)) == k)
        if len(candidate_itemsets) == 0:
            break
            
        # Count the frequency of each candidate itemset
        itemset_counts = {itemset: 0 for itemset in candidate_itemsets}
        for transaction in transactions:
            for itemset in candidate_itemsets:
                if itemset.issubset(transaction):
                    itemset_counts[itemset] += 1
        
        # Filter out infrequent itemsets
        frequent_itemsets = {itemset: count / len(transactions) for itemset, count in itemset_counts.items() if count / len(transactions) >= min_support}
        
        # Stop if there are no frequent itemsets of length k or more
        if len(frequent_itemsets) == 0:
            break
            
        # Add the frequent itemsets to the set of all frequent itemsets
        frequent_items.update(frequent_itemsets)
        
        # Increment k
        k += 1
        
    return frequent_items
transactions = [
    ['apple', 'banana', 'orange'],
    ['banana', 'orange', 'pear'],
    ['apple', 'banana', 'pear'],
    ['banana', 'pear'],
    ['apple', 'orange'],
    ['apple', 'banana', 'orange', 'pear']
]

frequent_itemsets = apriori(transactions, min_support=0.4)
print(frequent_itemsets)

/////////


import numpy as np
import pandas as pd

data = pd.read_csv('DMBI_prac2.csv')
print(data)

data['Average Score in ASIA'] = data['Average Score in ASIA'].fillna(data['Average Score in ASIA'].mean())
data['Average Score in SENA'] = data['Average Score in SENA'].fillna(data['Average Score in SENA'].mean())

data['Average Score in ASIA'] = data['Average Score in ASIA'].astype('int')
data['Average Score in SENA'] = data['Average Score in SENA'].astype('int')

print(data)




# equal frequency
def equifreq(arr1, m):	
	a = len(arr1)
	n = int(a / m)
	for i in range(0, m):
		arr = []
		for j in range(i * n, (i + 1) * n):
			if j >= a:
				break
			arr = arr + [arr1[j]]
		print(arr)

x= data.iloc[:, 1].values
print("equal depth binning")
equifreq(x, 9)



# equal width
def equiwidth(arr1, m):
	a = len(arr1)
	w = int((max(arr1) - min(arr1)) / m)
	min1 = min(arr1)
	arr = []
	for i in range(0, m + 1):
		arr = arr + [min1 + w * i]
	arri=[]
	
	for i in range(0, m):
		temp = []
		for j in arr1:
			if j >= arr[i] and j <= arr[i+1]:
				temp += [j]
		arri += [temp]
	print(arri)

x= data.iloc[:, 1].values

print("equal width binning")
equiwidth(x, 4)	


#smoothining 
# equi-depth 
# bin means method
bn=data.iloc[:,1].values
print(bn)
k = 3
for i in range(0,29,k):
  # mean of k values
  print("\nBin before")
  mean = 0
  for j in range(k):
    mean+=bn[i+j]
    print(bn[i+j],end=" ")
  
  mean = float(mean/k)
  print("\nBin after")
  
  for j in range(k):
   bn[i+j] = mean
   print(bn[i+j],end=" ")
  print()


#bin bounaries
temp =[]
for i in data["Average Score in ASIA"]:
  temp.append(i)

temp.sort()

# print("Before ",temp)

for i in range(0,29,k):
# at every bin store first and last values temp[i]
 mn = temp[i+k-1]
 mx = temp[i]
#depending upon distance change values
for j in range(k):
 if j >= k-1-j:
    temp[i+j] =mx

else:

 temp[i+j] ==mn

print("Smoothning by boundries ",temp)


# Max-Min Transformation
def max_min_transform(col):
    return (col - col.min()) / (col.max() - col.min())

data['Average Score in ASIA_max_min'] = max_min_transform(data['Average Score in ASIA'])

# Z-Score Transformation
def z_score_transform(col):
    return (col - col.mean()) / col.std()

data['Average Score in ASIA_z_score'] = z_score_transform(data['Average Score in ASIA'])



import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

data = pd.read_csv('hotel_37.csv')
print(data)



# Decimal Scaling Transformation
def decimal_scaling_transform(col):
    return col / (10 ** np.ceil(np.log10(col.abs().max())))

data['Average Score in ASIA_decimal_scaled'] = decimal_scaling_transform(data['Average Score in ASIA'])
print(data[['Average Score in ASIA','Average Score in ASIA_max_min','Average Score in ASIA_z_score','Average Score in ASIA_decimal_scaled']])



import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

data = pd.read_csv('hotel_37.csv')
print(data)

import numpy
from numpy import percentile
import matplotlib.pyplot as plt
from pandas import read_csv

data = read_csv('hotel_37.csv')
x = data.iloc[:,2]
quartile = percentile(data.iloc[:,2],[25,50,75])
min_data ,max_data = data.iloc[:,2].min(), data.iloc[:,2].max()

print('Five number summary:')
print('Min : %.1f'% min_data)
print('Q1: %.1f' % quartile[0])
print('Median: %.1f' % quartile[1])
print('Q3: %.1f' % quartile[2])
print('Max : %.1f'% max_data)


plt.boxplot(x)
plt.show()

plt.hist(x)
plt.show()

y = data.iloc[:,1]
plt.scatter(x,y)
plt.show()

import statsmodels.api as sm
import pylab as py
sm.qqplot(x, line ='45')
sm.qqplot(y, line ='45')
py.show()


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

data = pd.read_csv('subject_37.csv')
print(data)

observed = []
expected = []
for i in range(0,4):
  for j in range(1,4):
    a = data.iloc[i,j]
    b = ((data.iloc[i,4]*data.iloc[4,j])/data.iloc[4,4])
    observed.append(a)
    expected.append(b)
    
print('Observed values are:',observed)
print('Expected values are:',expected)

cnt = 0
chisq=0
for i in range(0,4):
  for j in range(1,4):
    chisq+= pow(((data.iloc[i,j])-expected[cnt]),2)/expected[cnt]
    cnt+=1
print('Chisquare value: ',chisq)


import pandas as pd
from scipy.stats import pearsonr
import csv

df = pd.read_csv("jobs_37.csv")
print(df)


mean1 = df['no of jobs'].mean()
mean2 = df['no of colleges'].mean()
std1 = df['no of jobs'].std()
std2 = df['no of colleges'].std()
cnt1 = df['no of jobs'].count()

attr1 = []
for i in range(0,10,1):
  for j in range(0,1):  
    a  = df.iloc[i,j]
    attr1.append(a)

attr2 =[]
for i in range(0,10,1):
  for j in range(1,2):  
    b  = df.iloc[i,j]
    attr2.append(b)
count=0
corr=0
for i in range(0, 10, 1):
  for j in range(0, 2):
    if count >= len(attr1):
      break
    corr += (attr1[count] - mean1) * (attr2[count] - mean2)
    count += 1
  if count >= len(attr1):
    break
r= corr/((cnt1-1)*std1*std2)

print('Pearson correlation value: ', r)
if r < 0 :
  print('The two attributes are negatively correlated')
elif r == 0 :
  print('The two attributes are independent')
else:
  print('The two attributes are positively correlated')

import numpy as np
from scipy.stats import pearsonr

# Create some data
x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 6, 8, 10])

# Calculate Pearson's r
r, p_value = pearsonr(x, y)

# Print the result
print("Pearson's r:", r)




